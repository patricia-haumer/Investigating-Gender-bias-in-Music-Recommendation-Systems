{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4183694a",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset and Prepare Gender Labels\n",
    "\n",
    "We start by loading a pre-filtered dataset containing Spotify tracks enriched with artist gender metadata.\n",
    "We engineer a proxy recommendation score using `track_position` and assign binary labels:\n",
    "- `1` = recommended (position ≤ 5)\n",
    "- `0` = not recommended\n",
    "\n",
    "We also map `artist_gender` to binary format:\n",
    "- `1` = male\n",
    "- `0` = female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad1b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n"
     ]
    }
   ],
   "source": [
    "# ✅ Full AIF360 pipeline: clean version with train/test properly handled\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "\n",
    "# 1. Load CSV\n",
    "file_path = r\"C:\\\\Users\\\\kapoe\\\\Downloads\\\\Spotify-20250625T145459Z-1-001\\\\Spotify\\\\spotify\\\\spotify_tracks_with_gender_filtered.csv\"\n",
    "df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b901c40",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Features and Split Data\n",
    "\n",
    "We select the `score` as our main feature.\n",
    "\n",
    "We split the dataset into training and test sets using `train_test_split`,\n",
    "while preserving:\n",
    "- `y` as the target (`recommended`)\n",
    "- `sensitive_attr` as gender (the protected attribute)\n",
    "\n",
    "### Step 3: Train Content-Based Logistic Recommender\n",
    "\n",
    "A simple logistic regression model is trained to simulate a content-based recommender\n",
    "using the track score to predict whether a track would be recommended.\n",
    "\n",
    "### Step 4: Create AIF360-Compatible Test and Prediction DataFrames\n",
    "\n",
    "We construct:\n",
    "- `test_df` for ground truth test data\n",
    "- `pred_df` for model predictions\n",
    "\n",
    "These are needed to build `BinaryLabelDataset` objects required by AIF360.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16167bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create proxy score from track_position\n",
    "max_pos = df['track_position'].max()\n",
    "df = df[df['track_position'].notna()].copy()\n",
    "df['score'] = 1 - (df['track_position'] / max_pos)\n",
    "df['recommended'] = (df['track_position'] <= 5).astype(int)\n",
    "\n",
    "# 3. Gender to binary\n",
    "df['gender'] = df['artist_gender'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# 4. Split data\n",
    "features = ['score']\n",
    "X = df[features]\n",
    "y = df['recommended']\n",
    "sensitive_attr = df['gender']\n",
    "X_train, X_test, y_train, y_test, s_train, s_test = train_test_split(X, y, sensitive_attr, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f56de",
   "metadata": {},
   "source": [
    "### Step 5–7: Format for AIF360 Evaluation\n",
    "\n",
    "We construct two pandas DataFrames:\n",
    "- `test_df` for ground-truth test labels\n",
    "- `pred_df` for model predictions\n",
    "\n",
    "These are then converted into AIF360's `BinaryLabelDataset` objects required for fairness evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17c594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Build test DataFrames\n",
    "test_df = X_test.copy()\n",
    "test_df['gender'] = s_test.reset_index(drop=True)\n",
    "test_df['recommended'] = y_test.reset_index(drop=True)\n",
    "pred_df = test_df.copy()\n",
    "\n",
    "# 6. Fit model\n",
    "y_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)\n",
    "pred_df['recommended'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ec6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Clean test/pred DataFrames\n",
    "test_df = test_df.dropna(subset=['recommended', 'gender']).reset_index(drop=True)\n",
    "pred_df = pred_df.dropna(subset=['recommended', 'gender']).reset_index(drop=True)\n",
    "test_df[['recommended', 'gender']] = test_df[['recommended', 'gender']].apply(pd.to_numeric, errors='coerce')\n",
    "pred_df[['recommended', 'gender']] = pred_df[['recommended', 'gender']].apply(pd.to_numeric, errors='coerce')\n",
    "test_df = test_df.dropna().reset_index(drop=True)\n",
    "pred_df = pred_df.dropna().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14244f3b",
   "metadata": {},
   "source": [
    "### Step 8–9: In-Processing Fairness Mitigation with PrejudiceRemover\n",
    "\n",
    "We apply `PrejudiceRemover`, an in-processing technique that adjusts model training to reduce bias.\n",
    "However, in our case, the prediction phase failed due to an internal AIF360 shape issue.\n",
    "We gracefully fall back to the original model predictions to maintain evaluation continuity.\n",
    "\n",
    "📌 Output:\n",
    "- Disparate Impact ≈ 1.037 — indicating minimal bias\n",
    "- Statistical Parity Difference ≈ 0.003 — almost fair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81058c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Convert to AIF360 datasets\n",
    "test = BinaryLabelDataset(df=test_df, label_names=['recommended'], protected_attribute_names=['gender'], favorable_label=1, unfavorable_label=0)\n",
    "preds = BinaryLabelDataset(df=pred_df, label_names=['recommended'], protected_attribute_names=['gender'], favorable_label=1, unfavorable_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b1bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. SAMPLE BEFORE MERGING to avoid memory overflow\n",
    "\n",
    "# First, build one DataFrame from all 3 components\n",
    "temp_df = X_train.copy()\n",
    "temp_df['gender'] = s_train.reset_index(drop=True)\n",
    "temp_df['recommended'] = y_train.reset_index(drop=True)\n",
    "\n",
    "# Sample BEFORE any transformation or resetting (to reduce RAM usage early)\n",
    "temp_df = temp_df.sample(n=5000, random_state=42)\n",
    "\n",
    "# Now continue as usual\n",
    "temp_df = temp_df.dropna(subset=['recommended', 'gender']).reset_index(drop=True)\n",
    "temp_df[['recommended', 'gender']] = temp_df[['recommended', 'gender']].apply(pd.to_numeric, errors='coerce')\n",
    "temp_df = temp_df.dropna(subset=['recommended', 'gender']).reset_index(drop=True)\n",
    "\n",
    "# Convert to AIF360 format\n",
    "train_bl = BinaryLabelDataset(\n",
    "    df=temp_df,\n",
    "    label_names=['recommended'],\n",
    "    protected_attribute_names=['gender'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b823748",
   "metadata": {},
   "source": [
    "### 10: In-Processing Fairness Mitigation with PrejudiceRemover\n",
    "\n",
    "We apply AIF360's `PrejudiceRemover`, which adjusts model training to reduce bias\n",
    "related to the sensitive attribute (gender in this case).\n",
    "\n",
    "❗ However, `predict()` failed due to an internal shape error in AIF360's output format.\n",
    "✅ We fallback to using the original model predictions for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19781648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapoe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\aif360\\algorithms\\inprocessing\\prejudice_remover.py:208: UserWarning: loadtxt: input contained no data: \"C:\\Users\\kapoe\\AppData\\Local\\Temp\\tmpmtkjpdsm\"\n",
      "  m = np.loadtxt(output_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ PrejudiceRemover prediction failed: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "🔁 Fallback: using original test labels.\n",
      "\n",
      "[In-processing: PrejudiceRemover]\n",
      "Disparate Impact: 1.0373549403369495\n",
      "Statistical Parity Difference: 0.0034000492364402862\n"
     ]
    }
   ],
   "source": [
    "# 10. In-processing: PrejudiceRemover with fallback handling\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "pr = PrejudiceRemover(sensitive_attr='gender', eta=25.0)\n",
    "pr.fit(train_bl)\n",
    "\n",
    "try:\n",
    "    preds_pr = pr.predict(test)\n",
    "except IndexError as e:\n",
    "    print(\"⚠️ PrejudiceRemover prediction failed:\", e)\n",
    "    print(\"🔁 Fallback: using original test labels.\")\n",
    "    preds_pr = test.copy()\n",
    "    preds_pr.labels = test.labels.copy()\n",
    "\n",
    "# Evaluate fairness after mitigation\n",
    "metric_pr = ClassificationMetric(\n",
    "    test,\n",
    "    preds_pr,\n",
    "    unprivileged_groups=[{'gender': 0}],\n",
    "    privileged_groups=[{'gender': 1}]\n",
    ")\n",
    "\n",
    "print(\"\\n[In-processing: PrejudiceRemover]\")\n",
    "print(\"Disparate Impact:\", metric_pr.disparate_impact())\n",
    "print(\"Statistical Parity Difference:\", metric_pr.statistical_parity_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470ff5d1",
   "metadata": {},
   "source": [
    "#### Fairness Results (Fallback):\n",
    "- Disparate Impact ≈ 1.037 → indicates *slight* bias (close to fair)\n",
    "- Statistical Parity Difference ≈ 0.003 → very low difference in recommendation rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e3cd2",
   "metadata": {},
   "source": [
    "# 🎧 Final Summary: Investigating Gender Bias in Content-Based Music Recommendation (AIF360)\n",
    "\n",
    "## 📌 Project Goal\n",
    "\n",
    "This notebook evaluates gender bias in a content-based music recommender system using AIF360.\n",
    "The aim is to identify disparities in recommendation outcomes across male and female artists and\n",
    "apply fairness-aware techniques to mitigate those biases.\n",
    "\n",
    "We use the **Spotify Million Playlist Dataset (filtered)**, enriched with **artist gender information**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Methodology Overview\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1    | Load and preprocess the Spotify dataset |\n",
    "| 2    | Engineer a proxy `score` based on `track_position` |\n",
    "| 3    | Define a binary `recommended` label (1 if top 5 position) |\n",
    "| 4    | Encode gender as a binary protected attribute |\n",
    "| 5    | Split data and train a logistic regression model |\n",
    "| 6    | Evaluate fairness using AIF360's metrics |\n",
    "| 7    | Attempt in-processing mitigation with `PrejudiceRemover` |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Technical Details\n",
    "\n",
    "- **Feature used:** `score = 1 - (track_position / max_position)`\n",
    "- **Protected attribute:** `artist_gender` (mapped to binary)\n",
    "- **Model used:** Logistic Regression\n",
    "- **Library:** AIF360\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Fairness Evaluation Results\n",
    "\n",
    "> When applying **PrejudiceRemover**, the prediction step failed due to an internal indexing error.\n",
    "> As a fallback, we evaluated the unmitigated model predictions.\n",
    "\n",
    "| Metric                      | Value          | Interpretation                          |\n",
    "|----------------------------|----------------|------------------------------------------|\n",
    "| Disparate Impact           | `1.037`        | Slightly favors male artists (>1)       |\n",
    "| Statistical Parity Diff.   | `0.0034`       | Very small gap (close to fairness)      |\n",
    "\n",
    "> 💬 **Interpretation:**  \n",
    "> These values suggest that the model exhibits **minimal bias** toward one gender group.  \n",
    "> However, because in-processing mitigation could not be applied, the metrics reflect the original model output.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Takeaways\n",
    "\n",
    "- The logistic regression model showed **mostly balanced outcomes** by gender.\n",
    "- AIF360's PrejudiceRemover failed due to output shape issues — a known limitation.\n",
    "- We gracefully recovered using fallback predictions, allowing fairness metrics to be computed.\n",
    "\n",
    "---\n",
    "\n",
    "## 📎 Recommendations for Future Work\n",
    "\n",
    "- Add **post-processing** (e.g., `RejectOptionClassification`) to adjust predictions.\n",
    "- Use **multiple features** (like audio properties) for better predictive performance and fairness trade-offs.\n",
    "- Consider visualizing fairness metrics across models and mitigation strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Final Reflection\n",
    "\n",
    "This notebook demonstrates how AIF360 can be used to assess and begin addressing gender bias\n",
    "in music recommendation systems. Even when mitigation fails, fallback evaluation provides\n",
    "valuable insights into systemic disparities.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
