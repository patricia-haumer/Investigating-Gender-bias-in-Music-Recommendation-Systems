{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497de3ca",
   "metadata": {},
   "source": [
    "# AIF360 Fairness Evaluation for Spotify Baseline Recommender\n",
    "\n",
    "This script analyzes fairness in a popularity-based music recommender system using the AIF360 toolkit. The dataset is derived from Spotify playlists with known artist gender labels.\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "\n",
    "### 1. **Data Loading**\n",
    "\n",
    "The CSV file `spotify_tracks_with_gender_filtered.csv` is loaded. It includes playlist track entries with `track_uri` and `artist_gender`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda7d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIF360 Baseline Fairness Analysis for Spotify Recommender\n",
    "\n",
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "# 1. Load the dataset\n",
    "tracks_df = pd.read_csv(\"spotify_tracks_with_gender_filtered.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435a7e9",
   "metadata": {},
   "source": [
    "### 2. **Track Popularity Calculation**\n",
    "\n",
    "Popularity is determined by counting how often each track appears across all playlists.\n",
    "\n",
    "```python\n",
    "popularity_df = tracks_df.groupby(\"track_uri\").size().reset_index(name=\"play_count\")\n",
    "```\n",
    "\n",
    "Then, we join this with gender information:\n",
    "\n",
    "```python\n",
    "track_gender_df = tracks_df.drop_duplicates(\"track_uri\")[[\"track_uri\", \"artist_gender\"]]\n",
    "popular_tracks = popularity_df.merge(track_gender_df, on=\"track_uri\")\n",
    "```\n",
    "\n",
    "\n",
    "### 3. **Top-N Recommendations**\n",
    "\n",
    "We sort by popularity and select the top 100 tracks as the recommender's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbe254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create popularity measure (count appearances in playlists)\n",
    "popularity_df = tracks_df.groupby(\"track_uri\").size().reset_index(name=\"play_count\")\n",
    "track_gender_df = tracks_df.drop_duplicates(\"track_uri\")[[\"track_uri\", \"artist_gender\"]]\n",
    "popular_tracks = popularity_df.merge(track_gender_df, on=\"track_uri\")\n",
    "\n",
    "# 3. Recommend top-N tracks\n",
    "N = 100\n",
    "popular_tracks_sorted = popular_tracks.sort_values(by=\"play_count\", ascending=False).reset_index(drop=True)\n",
    "recommendations = popular_tracks_sorted.head(N).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66613bd",
   "metadata": {},
   "source": [
    "### 4. **Labeling for Fairness Analysis**\n",
    "\n",
    "A binary column `recommended` is added: 1 if the track is in the top 100, 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044e7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Label dataset for fairness (1 if in top N, else 0)\n",
    "track_gender_df[\"recommended\"] = track_gender_df[\"track_uri\"].isin(recommendations[\"track_uri\"]).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8912ec4",
   "metadata": {},
   "source": [
    "### 5. **Filtering and Encoding Gender**\n",
    "\n",
    "Only male/female artists are retained. Gender is encoded numerically:\n",
    "\n",
    "* Female → 1 (unprivileged)\n",
    "* Male → 0 (privileged)\n",
    "\n",
    "Only numeric columns are kept to avoid AIF360 parsing issues:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a7ef49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kapoe\\AppData\\Local\\Temp\\ipykernel_3004\\3518838179.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gender_filtered[\"gender_binary\"] = gender_filtered[\"artist_gender\"].map(gender_map)\n"
     ]
    }
   ],
   "source": [
    "# 5. Keep only male/female for binary fairness analysis\n",
    "gender_filtered = track_gender_df[track_gender_df[\"artist_gender\"].isin([\"male\", \"female\"])]\n",
    "gender_map = {\"female\": 1, \"male\": 0}\n",
    "gender_filtered[\"gender_binary\"] = gender_filtered[\"artist_gender\"].map(gender_map)\n",
    "\n",
    "\n",
    "# Remove non-numeric columns before passing to AIF360\n",
    "aif_input_df = gender_filtered[[\"recommended\", \"gender_binary\"]].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac42b44",
   "metadata": {},
   "source": [
    "### 6. **Conversion to AIF360 Dataset Format**\n",
    "\n",
    "We convert to AIF360's `BinaryLabelDataset`, specifying:\n",
    "\n",
    "* Label: `recommended`\n",
    "* Protected attribute: `gender_binary`\n",
    "* Favorable label: 1 (recommended)\n",
    "* Unfavorable label: 0 (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5571289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Convert to BinaryLabelDataset\n",
    "aif_data = BinaryLabelDataset(\n",
    "    df=aif_input_df,\n",
    "    label_names=[\"recommended\"],\n",
    "    protected_attribute_names=[\"gender_binary\"],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e982d6b5",
   "metadata": {},
   "source": [
    "### 7. **Fairness Metrics Calculation**\n",
    "\n",
    "We calculate fairness metrics:\n",
    "\n",
    "* **Statistical Parity Difference**: Difference in recommendation rates between genders.\n",
    "* **Disparate Impact**: Ratio of recommendation rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48111c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fairness Metrics on Baseline Recommender ---\n",
      "Statistical parity difference: -0.0007922598070604304\n",
      "Disparate impact: 0.25370867405252645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Compute fairness metrics\n",
    "metric_orig = BinaryLabelDatasetMetric(\n",
    "    aif_data,\n",
    "    privileged_groups=[{\"gender_binary\": 0}],\n",
    "    unprivileged_groups=[{\"gender_binary\": 1}]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fairness Metrics on Baseline Recommender ---\")\n",
    "print(\"Statistical parity difference:\", metric_orig.statistical_parity_difference())\n",
    "print(\"Disparate impact:\", metric_orig.disparate_impact())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0984a0f8",
   "metadata": {},
   "source": [
    "### 8. **Bias Mitigation with Reweighing**\n",
    "A preprocessing technique that adjusts instance weights to reduce bias before training or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38cf3917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fairness Metrics After Reweighing ---\n",
      "Statistical parity difference: 1.0842021724855044e-19\n",
      "Disparate impact: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "# 8. (Optional) Apply reweighing\n",
    "rw = Reweighing(\n",
    "    privileged_groups=[{\"gender_binary\": 0}],\n",
    "    unprivileged_groups=[{\"gender_binary\": 1}]\n",
    ")\n",
    "rw.fit(aif_data)\n",
    "aif_data_rw = rw.transform(aif_data)\n",
    "\n",
    "metric_rw = BinaryLabelDatasetMetric(\n",
    "    aif_data_rw,\n",
    "    privileged_groups=[{\"gender_binary\": 0}],\n",
    "    unprivileged_groups=[{\"gender_binary\": 1}]\n",
    ")\n",
    "\n",
    "print(\"\\n--- Fairness Metrics After Reweighing ---\")\n",
    "print(\"Statistical parity difference:\", metric_rw.statistical_parity_difference())\n",
    "print(\"Disparate impact:\", metric_rw.disparate_impact())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e0adc",
   "metadata": {},
   "source": [
    "# Fairness Metrics Comparison Table\n",
    "\n",
    "| Metric                        | Baseline Recommender | After Reweighing |\n",
    "| ----------------------------- | -------------------- | ---------------- |\n",
    "| Statistical Parity Difference | -0.00079             | \\~0.00000        |\n",
    "| Disparate Impact              | 0.254                | 1.000            |\n",
    "\n",
    "### Notes:\n",
    "\n",
    "* **Statistical Parity Difference (SPD)** closer to 0 indicates more fair treatment between groups.\n",
    "* **Disparate Impact (DI)** ideally should be close to 1. Values < 0.8 or > 1.25 usually indicate potential bias.\n",
    "* The reweighing technique effectively neutralized bias based on these fairness metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
