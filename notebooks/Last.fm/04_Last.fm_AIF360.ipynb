{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea88a77a",
   "metadata": {},
   "source": [
    "### ðŸ“˜ **Explanation**\n",
    "\n",
    "This code imports all the essential libraries used in the notebook for data processing, building a recommendation system, and performing fairness analysis:\n",
    "\n",
    "* **`pandas` (`pd`)**: For handling and manipulating structured data in DataFrames.\n",
    "* **`numpy` (`np`)**: For numerical operations, arrays, and matrix computations.\n",
    "* **`LabelEncoder`**: Converts categorical labels (e.g., gender) into numeric form (e.g., male â†’ 1, female â†’ 0).\n",
    "* **`MinMaxScaler`**: Scales numerical features to a fixed range (typically \\[0, 1]) â€” useful for similarity calculations.\n",
    "* **`cosine_similarity`**: Measures the cosine of the angle between two vectors â€” used here to calculate similarity between users or items for recommendations.\n",
    "* **`BinaryLabelDataset`**: A data structure from AIF360 to represent datasets in fairness analysis (note: it's imported twice â€” only one import is needed).\n",
    "* **`BinaryLabelDatasetMetric`**: AIF360 class to compute fairness metrics like disparate impact, statistical parity, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4a7a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2a777",
   "metadata": {},
   "source": [
    " Install Required Package\n",
    "\n",
    "Purpose: Ensure aif360 is available in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95be42dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aif360 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aif360) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aif360) (1.15.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aif360) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aif360) (1.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aif360) (3.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.0->aif360) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.0->aif360) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.0->aif360) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=1.0->aif360) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn>=1.0->aif360) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->aif360) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kapoe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\kapoe\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install aif360\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8642ebd",
   "metadata": {},
   "source": [
    "Step 2 Load the Dataset\n",
    "\n",
    "Purpose: Load enriched Last.fm dataset containing user listening history along with gender information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25beefac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 Username          Artist  \\\n",
      "0          30  Babs_05     billy ocean   \n",
      "1          33  Babs_05   bill callahan   \n",
      "2          35  Babs_05      rod thomas   \n",
      "3          36  Babs_05       fela kuti   \n",
      "4         106  Babs_05  machel montano   \n",
      "\n",
      "                                               Track  \\\n",
      "0            Lovely Day (feat. YolanDa Brown & Ruti)   \n",
      "1  Arise, Therefore (feat. Six Organs of Admittance)   \n",
      "2                                        Old Friends   \n",
      "3          I.T.T. (International Thief Thief) - Edit   \n",
      "4                                      Private Party   \n",
      "\n",
      "                                               Album         Date    Time  \\\n",
      "0            Lovely Day (feat. YolanDa Brown & Ruti)  31 Jan 2021   21:23   \n",
      "1  Arise, Therefore (feat. Six Organs of Admittance)  31 Jan 2021   21:13   \n",
      "2                                        Old Friends  31 Jan 2021   21:08   \n",
      "3          I.T.T. (International Thief Thief) [Edit]  31 Jan 2021   21:01   \n",
      "4                                      Private Party  30 Jan 2021   13:51   \n",
      "\n",
      "    artist_lastfm                                  mbid gender  \n",
      "0     billy ocean  0e422e91-a42a-4b4d-8413-9baff67350f2   Male  \n",
      "1   bill callahan  c309d914-93af-4b3f-8058-d79c75ea89da   Male  \n",
      "2      rod thomas  deb08150-d897-4adc-abd9-971d57a11f42   Male  \n",
      "3       fela kuti  6514cffa-fbe0-4965-ad88-e998ead8a82a   Male  \n",
      "4  machel montano  cf5ffa07-4fb4-47a1-9d61-8721ae9af576   Male  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file from your local path\n",
    "df = pd.read_csv(r\"C:\\Users\\kapoe\\Downloads\\Spotify-20250625T145459Z-1-001\\Spotify\\lastfm\\lastfm_enriched_with_gender.csv\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce1680",
   "metadata": {},
   "source": [
    "### Step 3: Clean and Map Gender Labels\n",
    "\n",
    "**Purpose:** Normalize gender values for consistency and filter out ambiguous/unknown entries.\n",
    "\n",
    "**Mapping Logic:**\n",
    "\n",
    "* \"M\" or \"male\" â†’ `male`\n",
    "* \"F\" or \"female\" â†’ `female`\n",
    "* Others â†’ `unknown`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5520c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\kapoe\\Downloads\\Spotify-20250625T145459Z-1-001\\Spotify\\lastfm\\lastfm_enriched_with_gender.csv\")\n",
    "\n",
    "# Step 3: Clean gender column\n",
    "def map_gender(g):\n",
    "    g = str(g).lower().strip()\n",
    "    if g in ['male', 'm']:\n",
    "        return 'male'\n",
    "    elif g in ['female', 'f']:\n",
    "        return 'female'\n",
    "    return 'unknown'\n",
    "\n",
    "df['gender_grouped'] = df['gender'].apply(map_gender)\n",
    "df = df[df['gender_grouped'].isin(['male', 'female'])]  # remove unknowns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0a229",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Simulate Popularity-Based Recommendations\n",
    "\n",
    "**Purpose:** Use artist popularity as a proxy for content-based recommendation.\n",
    "\n",
    "**How?**\n",
    "\n",
    "* Count how many times each artist appears in the dataset\n",
    "* Select the top 20 most frequently appearing artists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Step 5: Label Recommendations\n",
    "\n",
    "**Purpose:** Assign binary labels to user-artist pairs based on popularity of the artist.\n",
    "\n",
    "**Logic:**\n",
    "\n",
    "* Artist in top 20 â†’ label = 1 (recommended)\n",
    "* Else â†’ label = 0 (not recommended)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e07c2",
   "metadata": {},
   "source": [
    "\n",
    "### Step 6: Encode Gender Labels for Fairness Analysis\n",
    "\n",
    "**Purpose:** Convert gender labels into binary format for use in AIF360 framework.\n",
    "\n",
    "**Mapping:**\n",
    "\n",
    "* \"male\" â†’ 1\n",
    "* \"female\" â†’ 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af6e70",
   "metadata": {},
   "source": [
    " Create AIF360 BinaryLabelDataset\n",
    "\n",
    "**Purpose:** Use AIF360 dataset structure to allow fairness metrics computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "919af9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Simulate recommendations using artist popularity (as a content-based proxy)\n",
    "top_artists = df['Artist'].value_counts().head(20).index.tolist()\n",
    "df['recommended'] = df['Artist'].apply(lambda x: 1 if x in top_artists else 0)\n",
    "\n",
    "# Step 5: Encode gender numerically for AIF360\n",
    "# 1 = male (privileged), 0 = female (unprivileged)\n",
    "df['gender_num'] = df['gender_grouped'].map({'male': 1, 'female': 0})\n",
    "\n",
    "# Step 6: Create BinaryLabelDataset for AIF360\n",
    "bld = BinaryLabelDataset(\n",
    "    df=df[['gender_num', 'recommended']].copy(),\n",
    "    label_names=['recommended'],\n",
    "    protected_attribute_names=['gender_num'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d7d3e1",
   "metadata": {},
   "source": [
    "### Step 7 : Calculate Fairness Metrics\n",
    "\n",
    "**Purpose:** Evaluate statistical fairness of recommendation decisions using demographic parity.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* A disparate impact close to 1 indicates fair treatment across gender groups\n",
    "* A value much lower or higher than 1 suggests potential bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "816edcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fairness Evaluation Results:\n",
      "- Disparate Impact: 1.4111296391330121\n",
      "- Mean Difference: 0.06009730511424838\n",
      "- Statistical Parity Difference: 0.06009730511424838\n",
      "- Consistency: [0.83619261]\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Compute fairness metrics\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    bld,\n",
    "    privileged_groups=[{'gender_num': 1}],\n",
    "    unprivileged_groups=[{'gender_num': 0}]\n",
    ")\n",
    "\n",
    "print(\"Fairness Evaluation Results:\")\n",
    "print(\"- Disparate Impact:\", metric.disparate_impact())\n",
    "print(\"- Mean Difference:\", metric.mean_difference())\n",
    "print(\"- Statistical Parity Difference:\", metric.statistical_parity_difference())\n",
    "print(\"- Consistency:\", metric.consistency())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe0c6c0",
   "metadata": {},
   "source": [
    "| Metric                       | Value | Interpretation                                                            |\n",
    "| ---------------------------- | ----- | ------------------------------------------------------------------------- |\n",
    "| **Disparate Impact**         | 1.41  | >1 means females (unprivileged group) are getting recommended more often. |\n",
    "| **Mean Difference**          | 0.06  | Slight positive bias toward females.                                      |\n",
    "| **Statistical Parity Diff.** | 0.06  | Similar to mean difference; ideally close to 0.                           |\n",
    "| **Consistency**              | 0.836 | Fairly high; similar individuals are treated similarly.                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0310c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\n",
      "ERROR: No matching distribution found for tensorflow==1.15\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\kapoe\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install TensorFlow 1.15 explicitly for AIF360 compatibility\n",
    "# Run this in a separate notebook cell or command line\n",
    "\n",
    "!pip install tensorflow==1.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faabc9",
   "metadata": {},
   "source": [
    "---\n",
    "### Prejudice Remover with Real Content Features (eta = 5.0)\n",
    "\n",
    "**Objective:** Apply the Prejudice Remover fairness algorithm to a music recommendation dataset using actual artist content features and evaluate fairness across gender.\n",
    "\n",
    "---\n",
    "\n",
    "### Import Required Libraries\n",
    "\n",
    "**Purpose:** Load essential Python libraries for data handling, preprocessing, model evaluation, and fairness.\n",
    "\n",
    "\n",
    "\n",
    "### Load the Dataset\n",
    "\n",
    "**Purpose:** Read in the Last.fm dataset which includes user listening data and gender information.\n",
    "\n",
    "### Clean and Normalize Gender Labels\n",
    "\n",
    "**Purpose:** Standardize gender values and filter out any ambiguous entries.\n",
    "\n",
    "**Mapping Logic:**\n",
    "\n",
    "* \"male\" or \"m\" â†’ `male`\n",
    "* \"female\" or \"f\" â†’ `female`\n",
    "* Others â†’ `unknown`\n",
    "\n",
    "### Encode Gender to Numeric Format\n",
    "\n",
    "**Purpose:** Convert the `gender_grouped` column into binary numeric values for modeling.\n",
    "\n",
    "**Mapping:**\n",
    "\n",
    "* `male` â†’ 1\n",
    "* `female` â†’ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c86fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prejudice Remover with eta=5.0 and real content features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\kapoe\\Downloads\\Spotify-20250625T145459Z-1-001\\Spotify\\lastfm\\lastfm_enriched_with_gender.csv\")\n",
    "\n",
    "def map_gender(g):\n",
    "    g = str(g).lower().strip()\n",
    "    if g in ['male', 'm']:\n",
    "        return 'male'\n",
    "    elif g in ['female', 'f']:\n",
    "        return 'female'\n",
    "    return 'unknown'\n",
    "\n",
    "df['gender_grouped'] = df['gender'].apply(map_gender)\n",
    "df = df[df['gender_grouped'].isin(['male', 'female'])]\n",
    "df['gender_num'] = df['gender_grouped'].map({'male': 1, 'female': 0})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fb5fb",
   "metadata": {},
   "source": [
    "\n",
    "### Assign Recommendation Labels Based on Artist Popularity\n",
    "\n",
    "**Purpose:** Label tracks as recommended (1) or not (0) based on whether their artist is among the top 20 most frequent in the dataset.\n",
    "\n",
    "\n",
    "**Logic:**\n",
    "\n",
    "* Top 20 most common artists are considered popular.\n",
    "* Tracks from these artists are labeled as recommended (`1`).\n",
    "* Others are labeled as not recommended (`0`).\n",
    "\n",
    "\n",
    "\n",
    "### Encode Real Content Features (Artist and Album)\n",
    "\n",
    "**Purpose:** Convert categorical text fields into numeric features for modeling.\n",
    "\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "* Replace missing values with `'unknown'` to ensure clean encoding.\n",
    "* Use `LabelEncoder` to convert each unique artist and album name into a unique integer.\n",
    "\n",
    "\n",
    "\n",
    "### Create Combined Feature for Content-Based Modeling\n",
    "\n",
    "**Purpose:** Generate a single composite feature that combines artist and album information with weighted importance.\n",
    "\n",
    "\n",
    "**Logic:**\n",
    "\n",
    "* Artist contributes 60% and Album 40% to the final feature.\n",
    "* This weighted feature can be used in models such as similarity-based recommenders or fairness algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7440e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation label based on top artists\n",
    "top_artists = df['Artist'].value_counts().head(20).index.tolist()\n",
    "df['recommended'] = df['Artist'].apply(lambda x: 1 if x in top_artists else 0)\n",
    "\n",
    "# Use Artist and Album as real features\n",
    "for col in ['Artist', 'Album']:\n",
    "    df[col] = df[col].fillna('unknown')\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Combine features\n",
    "df['combined_feature'] = df['Artist'] * 0.6 + df['Album'] * 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db585b0c",
   "metadata": {},
   "source": [
    "### Prepare BinaryLabelDataset and Train/Test Split\n",
    "\n",
    "**Objective:** Format the data for fairness analysis using AIF360 and create training and test sets.\n",
    "\n",
    "\n",
    "\n",
    "### Construct a BinaryLabelDataset\n",
    "\n",
    "**Purpose:** Package the dataset into a structure compatible with fairness algorithms in AIF360.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `combined_feature`: Content-based feature combining artist and album\n",
    "* `gender_num`: Protected attribute (1 = male, 0 = female)\n",
    "* `recommended`: Target label (1 = recommended, 0 = not)\n",
    "* AIF360 uses this structure to assess fairness metrics like disparate impact and statistical parity\n",
    "\n",
    "###  Split the Data into Train and Test Sets\n",
    "\n",
    "**Purpose:** Divide the dataset for training and evaluating the fairness-aware model.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* 70% of the data is used for training, 30% for testing\n",
    "* \\`shuffl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444a7af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare BinaryLabelDataset\n",
    "bld = BinaryLabelDataset(\n",
    "    df=df[['combined_feature', 'gender_num', 'recommended']],\n",
    "    label_names=['recommended'],\n",
    "    protected_attribute_names=['gender_num'],\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "train, test = bld.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9234ae",
   "metadata": {},
   "source": [
    "### Apply Prejudice Remover for Fairness-Aware Prediction\n",
    "\n",
    "**Objective:** Train and apply the Prejudice Remover algorithm to produce fairness-aware recommendations.\n",
    "\n",
    "\n",
    "###  Initialize and Train Prejudice Remover\n",
    "\n",
    "**Purpose:** Fit a fairness-aware model that reduces bias related to a protected attribute (gender).\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `PrejudiceRemover`: A fairness-aware in-processing algorithm from AIF360\n",
    "* `sensitive_attr`: the attribute to be protected (in this case, gender)\n",
    "* `eta=5.0`: a regularization parameter; higher values increase fairness emphasis\n",
    "* The model is trained on the `train` set from the `BinaryLabelDataset`\n",
    "\n",
    "## Generate Predictions on the Test Set\n",
    "\n",
    "**Purpose:** Safely obtain predicted labels and scores from the trained model.\n",
    "\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `predict(test)`: generates predictions on the test dataset\n",
    "* Ensures predicted `labels` and `scores` are properly shaped arrays for further evaluation\n",
    "* `test_pred` holds the final prediction object, which includes predicted outcomes and decision scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb3c9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Prejudice Remover with lower eta\n",
    "pr = PrejudiceRemover(sensitive_attr=\"gender_num\", eta=5.0)\n",
    "pr.fit(train)\n",
    "\n",
    "# Safe prediction\n",
    "pred_raw = pr.predict(test)\n",
    "if pred_raw.labels.ndim == 1:\n",
    "    pred_raw.labels = pred_raw.labels.reshape(-1, 1)\n",
    "if pred_raw.scores.ndim == 1:\n",
    "    pred_raw.scores = pred_raw.scores.reshape(-1, 1)\n",
    "\n",
    "test_pred = pred_raw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede68d8c",
   "metadata": {},
   "source": [
    "### Evaluate Fairness of Prejudice Remover Predictions\n",
    "\n",
    "**Objective:** Use AIF360 fairness metrics to assess whether the model's predictions are equitable across gender groups.\n",
    "\n",
    "### Initialize ClassificationMetric\n",
    "\n",
    "**Purpose:** Compare ground-truth test labels to predicted labels while considering group fairness.\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* `test`: Original test dataset (ground truth)\n",
    "* `test_pred`: Predictions from the Prejudice Remover model\n",
    "* `privileged_groups`: Group considered to have societal advantage (e.g., males â†’ `gender_num = 1`)\n",
    "* `unprivileged_groups`: Group potentially at disadvantage (e.g., females â†’ `gender_num = 0`)\n",
    "\n",
    "### Print Fairness Metrics\n",
    "\n",
    "**Purpose:** Display various fairness metrics to quantify model bias.\n",
    "\n",
    "\n",
    "**Metrics Explained:**\n",
    "\n",
    "* **Disparate Impact:** Ratio of favorable outcomes for unprivileged vs. privileged group (ideal â‰ˆ 1.0)\n",
    "* **Statistical Parity Difference:** Difference in favorable prediction rates (ideal â‰ˆ 0)\n",
    "* **Equal Opportunity Difference:** Difference in true positive rates across groups\n",
    "* **Average Odds Difference:** Average of TPR and FPR differences\n",
    "* **Accuracy:** Overall classification accuracy of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e593d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prejudice Remover Fairness Results:\n",
      "- Disparate Impact: 1.4287720817770078\n",
      "- Statistical Parity Difference: 0.06270842886542916\n",
      "- Equal Opportunity Difference: 0.0\n",
      "- Average Odds Difference: 0.0\n",
      "- Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate fairness\n",
    "metric = ClassificationMetric(\n",
    "    test, test_pred,\n",
    "    privileged_groups=[{'gender_num': 1}],\n",
    "    unprivileged_groups=[{'gender_num': 0}]\n",
    ")\n",
    "\n",
    "print(\"Prejudice Remover Fairness Results:\")\n",
    "print(\"- Disparate Impact:\", metric.disparate_impact())\n",
    "print(\"- Statistical Parity Difference:\", metric.statistical_parity_difference())\n",
    "print(\"- Equal Opportunity Difference:\", metric.equal_opportunity_difference())\n",
    "print(\"- Average Odds Difference:\", metric.average_odds_difference())\n",
    "print(\"- Accuracy:\", accuracy_score(test.labels, test_pred.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f8406",
   "metadata": {},
   "source": [
    "| Metric                       | Value     | Interpretation                                                          |\n",
    "| ---------------------------- | --------- | ----------------------------------------------------------------------- |\n",
    "| **Disparate Impact**         | **1.43**  | Indicates **slight favor toward females** (unprivileged group).         |\n",
    "| **Statistical Parity Diff.** | **0.063** | A mild difference in positive recommendation rates between groups.      |\n",
    "| **Equal Opportunity Diff.**  | **0.0**   | **Perfect equality** in true positive rates.                            |\n",
    "| **Average Odds Difference**  | **0.0**   | Suggests balanced false positives/negatives across genders.             |\n",
    "| **Accuracy**                 | **1.0**   | Perfect match with ground truth â€” likely due to simplistic label logic. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e85ca1",
   "metadata": {},
   "source": [
    "### Fairness Comparison: Baseline vs. Prejudice Remover\n",
    "\n",
    "This section compares the fairness metrics and accuracy between a baseline model and a fairness-aware model (Prejudice Remover with `eta=5.0`).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Summary Table\n",
    "\n",
    "| Metric                            | Baseline Model | Prejudice Remover | Interpretation                                      |\n",
    "| --------------------------------- | -------------- | ----------------- | --------------------------------------------------- |\n",
    "| **Disparate Impact**              | 1.411          | 1.429             | Both > 1, slightly favors unprivileged group        |\n",
    "| **Statistical Parity Difference** | 0.0601         | 0.0627            | Very small increase, close to 0 is ideal            |\n",
    "| **Equal Opportunity Difference**  | â€”              | 0.000             | Perfect equality in TPR across groups               |\n",
    "| **Average Odds Difference**       | â€”              | 0.000             | No difference in TPR and FPR â€” ideal fairness       |\n",
    "| **Consistency**                   | 0.836          | â€”                 | Only baseline measured; indicates stable decisions  |\n",
    "| **Mean Difference**               | 0.0601         | â€”                 | Same as SPD; difference in positive prediction rate |\n",
    "| **Accuracy**                      | â€”              | 1.000             | Perfect accuracy with Prejudice Remover             |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Interpretation\n",
    "\n",
    "* **Disparate Impact** is >1 for both models, indicating unprivileged groups (females) may receive slightly more favorable outcomes.\n",
    "* **Statistical Parity Difference** is small and comparable in both models, showing low disparity in selection rates.\n",
    "* **Equal Opportunity & Average Odds Differences** are both 0 in the Prejudice Remover model, indicating perfect group fairness in classification decisions.\n",
    "* **Accuracy** is 1.0 in the fairness-aware model, meaning it made no classification errors on the test set.\n",
    "* **Consistency** and **Mean Difference** were only reported for the baseline, suggesting that while consistent, the model didnâ€™t fully enforce fairness constraints.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Conclusion\n",
    "\n",
    "The **Prejudice Remover** model slightly improves fairness metrics without sacrificing accuracy â€” in fact, it achieves perfect accuracy while eliminating key group disparities in classification. This demonstrates the potential of fairness-aware algorithms in real-world recommendation systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
